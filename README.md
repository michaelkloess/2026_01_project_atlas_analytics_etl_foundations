# ğŸ“• Project_Atlas_Analytics_ETL_Foundations

**ğŸ“Œ Project Overview**  
This project documents my work during the Webeet Internship Program, which I completed as part of my practical training as a Data Analyst. The project represents my end-to-end learning journey, covering a 5-day onboarding plan, exploratory data analysis, Python-based analytics, SQL querying via Python, and structured data integration into a AWS PostgreSQL database.

The goal of this project is to present the tasks I completed as a cohesive analytics project that reflects real-world data workflows and professional standards.

## ğŸ“• Business Context

### ğŸ“Œ Challenge
As a new intern, the main challenge was to quickly adapt to a professional data analytics environment. 
This included setting up the required tools, understanding unfamiliar datasets, and applying analytical methods under time constraints while following team standards for documentation and version control.

### ğŸ“Œ Solution
The internship program was structured into daily tasks that gradually increased in complexity. 
Through guided sessions and independent work, I applied Python, SQL, and database concepts to real datasets. 
Each task required clear documentation, reproducible analysis, and submission via GitHub Commits and Pull Requests.

### ğŸ“Œ Objectives
- Successfully complete all onboarding and analytical tasks  
- Apply Python and SQL to real-world datasets  
- Practice structured data exploration and cleaning  
- Work with a AWS PostgreSQL database via Python  
- Document results in a professional and reproducible way  

## ğŸ“• Technology Stack

### ğŸ“Œ Core Technologies
- **Database:** AWS PostgreSQL  
- **Programming:** SQL, Python  
- **Development:** Jupyter Notebook, VS Code  
- **Libraries:** Pandas, SQLAlchemy, Matplotlib  

### ğŸ“Œ Implementation Approach
- **Data Processing:** SQL queries executed via Python followed by Pandas analysis  
- **Segmentation:** Exploratory and rule-based analytical logic depending on task scope  
- **Documentation:** Markdown files and Jupyter Notebooks tracked with Git  

## ğŸ“ Project Structure

### ğŸ“Œ Root
- [README.md](./README.md) â†’ Project overview and goals  

---

### ğŸ“Œ 01_docs/ â†’ Business and project documentation
- [00_project_structure.md](./01_docs/00_project_structured.md) â†’ Overview of the internship task structure and repository layout  
- [01_business_use_case.md](./01_docs/01_business_use_case.md) â†’ Context and objectives of the internship tasks  
- [02_data_requirements.md](./01_docs/02_data_requirements.md) â†’ Description of datasets used throughout the project  
- [03_preparing_data.md](./01_docs/03_preparing_data.md) â†’ Data loading and preparation steps  
- [04_data_observations.md](./01_docs/04_data_observations.md) â†’ Observations made during exploratory analysis  
- [05_bias_injection_log.md](./01_docs/05_bias_injection_log.md) â†’ Notes on assumptions and data limitations  
- [06_deployment_next_steps.md](./01_docs/06_deployment_next_steps.md) â†’ Ideas for extending the project further  
- [07_glossary.md](./01_docs/07_glossary.md) â†’ Definitions of technical and domain-specific terms  

---

### ğŸ“Œ 02_data/ â†’ Organized data storage
- [01_raw/](./02_data/01_raw/) â†’ Just Raw Datasets without any modification
- [02_interim/](./02_data/02_interim/) â†’ Intermediate cleaned or transformed datasets 
- [03_processed/](./02_data/03_processed/) â†’ Final datasets ready to ship


---

### ğŸ“Œ 03_notebooks/ â†’ Jupyter notebooks organized by internship tasks
- [01_python_data_exploration.ipynb](./03_notebooks/01_python_data_exploration.ipynb) â†’ Python Data Exploration
- [02_sql_via_python.ipynb](./03_notebooks/02_sql_via_python.ipynb) â†’ SQL via Python: NYC School Data Exploration
- [03_populating_database.ipynb](./03_notebooks/03_populating_database.ipynb) â†’ Populating a AWS PostgreSQL Database 


---
 

## ğŸ“• Technical Implementation

### ğŸ“Œ Key Technologies
- **SQL:** Data extraction and aggregation  
- **Python:** Data cleaning, analysis, and visualization  
- **Jupyter Notebooks:** Interactive documentation and analysis  

## ğŸ“• Project Timeline

### ğŸ“Œ Development Phases {30.12.2025 â€“ 06.01.2026 | 4 Days}

**Day 1 â€“ Onboarding & Exploration:**  
- Verified access and environment setup 
- Explored a small dataset 
- Submitted findings as a Markdown file 

**Day 2 â€“ Python Analysis:**  
- Cleaned, analyzed and visualized data using Pandas + Matplotlib  
- Created basic visualizations  
- Documented insights in a notebook  

**Day 3 â€“ SQL via Python:**  
- Connected to a AWS PostgreSQL database  
- Executed SQL queries from Python  
- Analyzed and summarized results  

**Day 4 â€“ Data Integration:**  
- Inspected and cleaned a messy dataset  
- Prepared data for database insertion  
- Implemented a scripted ingestion workflow  

## ğŸ“• Current Status

### ğŸ“Œ Completion Overview
- ğŸŸ¡ **Documentation:** Complete  
- ğŸŸ¢ **Python Analysis:** Complete  
- ğŸŸ¢ **SQL Integration:** Complete  
- ğŸŸ¢ **Data Ingestion:** Complete  
- ğŸŸ¢ **Version Control:** Complete  

### ğŸ“Œ Key Deliverables
- Jupyter notebooks for each task  
- SQL queries executed via Python  
- Data cleaning and ingestion scripts  
- Markdown-based documentation  
- GitHub Commits and Pull Requests  

## ğŸ“• Next Steps

### ğŸ“Œ Immediate Priorities
1. Extend the analysis with additional datasets  
2. Automate validation and quality checks  
3. Improve visual reporting  
4. Prepare the project for production-like use  

### ğŸ“Œ Success Metrics
- Reproducibility of results  
- Code readability and structure  
- Correct database integration  
- Clear and documented analytical insights  

**Contact:** All task-specific details and discussions can be found in the corresponding repository documentation and GitHub issues.
